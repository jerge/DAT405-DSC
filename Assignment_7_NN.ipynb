{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment_7_NN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerge/DAT405-DSC/blob/main/Assignment_7_NN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rHoSDyYpdh-s"
      },
      "source": [
        "Assignment 7: Neural Networks using Keras and Tensorflow Please see the associated document for questions\n",
        "\n",
        "If you have problems with Keras and Tensorflow on your local installation please make sure they are updated. On Google Colab this notebook runs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "02ZYZ-WmdhwH"
      },
      "source": [
        "# imports\n",
        "from __future__ import print_function\n",
        "import keras\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras import backend as K\n",
        "import tensorflow as tf\n",
        "from matplotlib import pyplot as plt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BJRCoRmew8Zd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30f8128d-921a-4088-9e8d-ef920185d5ad"
      },
      "source": [
        "# Hyper-parameters data-loading and formatting\n",
        "\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 10\n",
        "\n",
        "img_rows, img_cols = 28, 28\n",
        "\n",
        "(x_train, lbl_train), (x_test, lbl_test) = mnist.load_data()\n",
        "\n",
        "if K.image_data_format() == 'channels_first':\n",
        "    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
        "    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
        "    input_shape = (1, img_rows, img_cols)\n",
        "else:\n",
        "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
        "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
        "    input_shape = (img_rows, img_cols, 1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-I3g1RrZ0wpI"
      },
      "source": [
        "**Preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XLxGm7ojsYLm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0a8595-2f4e-4314-f43b-3aa22544a0c9"
      },
      "source": [
        "y_test"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 1., ..., 0., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UswCCQLS0s1I"
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "y_train = keras.utils.to_categorical(lbl_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(lbl_test, num_classes)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Aer42gk1W9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "71e61263-71b8-4762-ee86-958825218ee7"
      },
      "source": [
        "\n",
        "## Define model ##\n",
        "model = Sequential()\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(64, activation = 'relu'))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "model.compile(loss=keras.losses.categorical_crossentropy,\n",
        "               optimizer=keras.optimizers.SGD(lr = 0.1),\n",
        "        metrics=['accuracy'],)\n",
        "\n",
        "fit_info = model.fit(x_train, y_train,\n",
        "           batch_size=batch_size,\n",
        "           epochs=epochs,\n",
        "           verbose=1,\n",
        "           validation_data=(x_test, y_test))\n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Test loss: {}, Test accuracy {}'.format(score[0], score[1]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "469/469 [==============================] - 1s 3ms/step - loss: 0.4958 - accuracy: 0.8586 - val_loss: 0.2576 - val_accuracy: 0.9260\n",
            "Epoch 2/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.2337 - accuracy: 0.9312 - val_loss: 0.2005 - val_accuracy: 0.9400\n",
            "Epoch 3/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.1800 - accuracy: 0.9485 - val_loss: 0.1624 - val_accuracy: 0.9524\n",
            "Epoch 4/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.1480 - accuracy: 0.9570 - val_loss: 0.1479 - val_accuracy: 0.9566\n",
            "Epoch 5/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.1264 - accuracy: 0.9630 - val_loss: 0.1309 - val_accuracy: 0.9610\n",
            "Epoch 6/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.1103 - accuracy: 0.9677 - val_loss: 0.1119 - val_accuracy: 0.9659\n",
            "Epoch 7/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0979 - accuracy: 0.9709 - val_loss: 0.1074 - val_accuracy: 0.9671\n",
            "Epoch 8/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0884 - accuracy: 0.9739 - val_loss: 0.1047 - val_accuracy: 0.9670\n",
            "Epoch 9/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0799 - accuracy: 0.9761 - val_loss: 0.0988 - val_accuracy: 0.9690\n",
            "Epoch 10/10\n",
            "469/469 [==============================] - 1s 2ms/step - loss: 0.0730 - accuracy: 0.9783 - val_loss: 0.0989 - val_accuracy: 0.9699\n",
            "Test loss: 0.09894514828920364, Test accuracy 0.9699000120162964\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j_8NOfLqoxXZ"
      },
      "source": [
        "### Question 1) Explain the data pre-processing high-lighted in the notebook.\r\n",
        "\r\n",
        "*text*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1iy_I6i7o9Z6"
      },
      "source": [
        "### Question 2) 4 points. Network model, training, and changing hyper-parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nv9GTA2ApCk1"
      },
      "source": [
        "#### A) How many layers does the network in the notebook have? How many neurons does each layer have? What activation functions and why are these appropriate for this application? What is the total number of parameters for the network? Why does the input and output layers have the dimensions they have?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nxlYXyiGpKou"
      },
      "source": [
        "#### B) What loss-function is used to train the network? What is the functional form (mathematical expression) of the loss function? and how should we interpret it? Why is it appropriate for the problem at hand?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRECWUwgpPUF"
      },
      "source": [
        "#### C) Train the network for 10 epochs and plot the training and validation accuracy for each epoch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orfj7eDOpUCI"
      },
      "source": [
        "#### D) Update model to implement a three-layer neural network where the hidden-layers has 500 and 300 hidden units respectively. Train for 40 epochs. What is the best validation accuracy you can achieve? Geoff Hinton claimed this network could reach a validation accuracy of 0.9847 (http://yann.lecun.com/exdb/mnist/) using weight decay. Implement weight decay on hidden units and train and select 5 regularization factors from 0.000001 to 0.001. Train 3 replicates networks for each regularization factor. Plot the final validation accuracy with standard deviation (computed from the replicates) as a function of the regularization factor. How close do you get to Hintons result? – If you do not get the same results, what factors may influence this? (hint: What information is not given by Hinton on the MNIST database that may influence Model training)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T0bRoMYqpTk1"
      },
      "source": [
        "# implement three-layer neural network"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-cdaoBWqqB69"
      },
      "source": [
        "### 3) 2 points. Convolutional layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxkewS-UqHw7"
      },
      "source": [
        "#### A) Design a model that makes use of at least one convolutional layer – how performant a model can you get? -- According to the MNIST database it should be possible reach to 99% accuracy on the validation data. If you choose to use any layers apart from convolutional layers and layers that you used in previous questions, you must describe what they do. If you do not reach 99% accuracy, report your best performance and explain your attempts and thought process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jMLULYE1qOcA"
      },
      "source": [
        "#### B) Discuss the differences and potential benefits of using convolutional layers over fully connected ones for the particular application?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I2Bkk_rhUnH"
      },
      "source": [
        "### Question 4) Auto-Encoder for denoising\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yO0HxKeJ7WFw"
      },
      "source": [
        "import numpy as np\n",
        "def salt_and_pepper(input, noise_level=0.5):\n",
        "    \"\"\"\n",
        "    This applies salt and pepper noise to the input tensor - randomly setting bits to 1 or 0.\n",
        "    Parameters\n",
        "    ----------\n",
        "    input : tensor\n",
        "        The tensor to apply salt and pepper noise to.\n",
        "    noise_level : float\n",
        "        The amount of salt and pepper noise to add.\n",
        "    Returns\n",
        "    -------\n",
        "    tensor\n",
        "        Tensor with salt and pepper noise applied.\n",
        "    \"\"\"\n",
        "    # salt and pepper noise\n",
        "    a = np.random.binomial(size=input.shape, n=1, p=(1 - noise_level))\n",
        "    b = np.random.binomial(size=input.shape, n=1, p=0.5)\n",
        "    c = (a==0) * b\n",
        "    return input * a + c\n",
        "\n",
        "\n",
        "#data preparation\n",
        "flattened_x_train = x_train.reshape(-1,784)\n",
        "flattened_x_train_seasoned = salt_and_pepper(flattened_x_train, noise_level=0.4)\n",
        "\n",
        "flattened_x_test = x_test.reshape(-1,784)\n",
        "flattened_x_test_seasoneed = salt_and_pepper(flattened_x_test, noise_level=0.4)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0GZtZH4ScQeN"
      },
      "source": [
        "\n",
        "latent_dim = 96  \n",
        "\n",
        "input_image = keras.Input(shape=(784,))\n",
        "encoded = Dense(128, activation='relu')(input_image)\n",
        "encoded = Dense(latent_dim, activation='relu')(encoded)\n",
        "decoded = Dense(128, activation='relu')(encoded)\n",
        "decoded = Dense(784, activation='sigmoid')(decoded)\n",
        "\n",
        "autoencoder = keras.Model(input_image, decoded)\n",
        "encoder_only = keras.Model(input_image, encoded)\n",
        "\n",
        "encoded_input = keras.Input(shape=(latent_dim,))\n",
        "decoder_layer = Sequential(autoencoder.layers[-2:])\n",
        "decoder = keras.Model(encoded_input, decoder_layer(encoded_input))\n",
        "\n",
        "autoencoder.compile(optimizer='adam', loss='binary_crossentropy')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "56iJOKNIKfuB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cfac3df-77a9-427c-d32f-1943eae94cfe"
      },
      "source": [
        "fit_info_AE = autoencoder.fit(flattened_x_train_seasoned, flattened_x_train,\n",
        "                epochs=32,\n",
        "                batch_size=64,\n",
        "                shuffle=True,\n",
        "                validation_data=(flattened_x_test_seasoneed, flattened_x_test))\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/32\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.1883 - val_loss: 0.1524\n",
            "Epoch 2/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1451 - val_loss: 0.1382\n",
            "Epoch 3/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1360 - val_loss: 0.1335\n",
            "Epoch 4/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1311 - val_loss: 0.1299\n",
            "Epoch 5/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1281 - val_loss: 0.1282\n",
            "Epoch 6/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1259 - val_loss: 0.1256\n",
            "Epoch 7/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1241 - val_loss: 0.1247\n",
            "Epoch 8/32\n",
            "938/938 [==============================] - 7s 7ms/step - loss: 0.1229 - val_loss: 0.1245\n",
            "Epoch 9/32\n",
            "938/938 [==============================] - 9s 9ms/step - loss: 0.1218 - val_loss: 0.1231\n",
            "Epoch 10/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1210 - val_loss: 0.1231\n",
            "Epoch 11/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1203 - val_loss: 0.1218\n",
            "Epoch 12/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1197 - val_loss: 0.1219\n",
            "Epoch 13/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1191 - val_loss: 0.1216\n",
            "Epoch 14/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1187 - val_loss: 0.1212\n",
            "Epoch 15/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1182 - val_loss: 0.1214\n",
            "Epoch 16/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1178 - val_loss: 0.1205\n",
            "Epoch 17/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1175 - val_loss: 0.1208\n",
            "Epoch 18/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1172 - val_loss: 0.1209\n",
            "Epoch 19/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1169 - val_loss: 0.1207\n",
            "Epoch 20/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1166 - val_loss: 0.1204\n",
            "Epoch 21/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1164 - val_loss: 0.1210\n",
            "Epoch 22/32\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.1162 - val_loss: 0.1202\n",
            "Epoch 23/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1160 - val_loss: 0.1200\n",
            "Epoch 24/32\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.1158 - val_loss: 0.1198\n",
            "Epoch 25/32\n",
            "938/938 [==============================] - 5s 5ms/step - loss: 0.1155 - val_loss: 0.1198\n",
            "Epoch 26/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1154 - val_loss: 0.1198\n",
            "Epoch 27/32\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.1152 - val_loss: 0.1208\n",
            "Epoch 28/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1150 - val_loss: 0.1198\n",
            "Epoch 29/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1149 - val_loss: 0.1197\n",
            "Epoch 30/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1147 - val_loss: 0.1193\n",
            "Epoch 31/32\n",
            "938/938 [==============================] - 5s 6ms/step - loss: 0.1147 - val_loss: 0.1201\n",
            "Epoch 32/32\n",
            "938/938 [==============================] - 6s 6ms/step - loss: 0.1146 - val_loss: 0.1194\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mr8NGwJ95Sf"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GeThq9jlqUb5"
      },
      "source": [
        "#### A) The notebook implements a simple denoising deep autoencoder model. Explain what the model does: use the data-preparation and model definition code to explain how the goal of the model is achieved. Explain the role of the loss function? Draw a diagram of the model and include it in your report. Train the model with the settings given."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ewYEhmOtqdYH"
      },
      "source": [
        "#### B) Add increasing levels of noise to the test-set using the salt_and_pepper()-function (0 to 1). Use matplotlib to visualize a few examples (3-4) in the original, “seasoned” (noisy), and denoised versions (Hint: for visualization use imshow(), use the trained autoencoder to denoise the noisy digits). At what noise level does it become difficult to identify the digits for you? At what noise level does the denoising stop working?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rqkStiFQqiRE"
      },
      "source": [
        "#### C) Test whether denoising improves the classification with the best performing model you obtained in questions 2 or 3. Plot the true-positive rate as a function of noise-level for the seasoned and denoised datasets – assume that the correct classification is the most likely class-label. Discuss your results. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uj0XkDZRqmbA"
      },
      "source": [
        "#### D) Explain how you can use the decoder part of the denoising auto-encoder to generate synthetic “hand-written” digits? – Describe the procedure and show examples in your report."
      ]
    }
  ]
}