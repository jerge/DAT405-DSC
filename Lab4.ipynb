{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Assignment4.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jerge/DAT405-DSC/blob/main/Lab4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-sTsDfIVKsmL"
      },
      "source": [
        "#DAT405 Introduction to Data Science and AI \n",
        "\n",
        "Daniel Willim: 13h\n",
        "\n",
        "Erik Jergéus: 13h\n",
        "\n",
        "##2020-2021, Reading Period 2\n",
        "## Assignment 4: Spam classification using Naïve Bayes \n",
        "There will be an overall grade for this assignment. To get a pass grade (grade 5), you need to pass items 1-3 below. To receive higher grades, finish items 4 and 5 as well. \n",
        "\n",
        "The exercise takes place in a notebook environment where you can chose to use Jupyter or Google Colabs. We recommend you use Google Colabs as it will facilitate remote group-work and makes the assignment less technical. \n",
        "Hints:\n",
        "You can execute certain linux shell commands by prefixing the command with `!`. You can insert Markdown cells and code cells. The first you can use for documenting and explaining your results the second you can use writing code snippets that execute the tasks required.  \n",
        "\n",
        "In this assignment you will implement a Naïve Bayes classifier in Python that will classify emails into spam and non-spam (“ham”) classes.  Your program should be able to train on a given set of spam and “ham” datasets. \n",
        "You will work with the datasets available at https://spamassassin.apache.org/old/publiccorpus/. There are three types of files in this location: \n",
        "-\teasy-ham: non-spam messages typically quite easy to differentiate from spam messages. \n",
        "-\thard-ham: non-spam messages more difficult to differentiate \n",
        "-\tspam: spam messages \n",
        "\n",
        "**Execute the cell below to download and extract the data into the environment of the notebook -- it will take a few seconds.** If you chose to use Jupyter notebooks you will have to run the commands in the cell below on your local computer, with Windows you can use 7zip (https://www.7-zip.org/download.html) to decompress the data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wa37fBwRF-xe",
        "outputId": "6a65d37d-45fb-4361-b957-067582013381"
      },
      "source": [
        "#Download and extract data\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
        "!wget https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
        "!tar -xjf 20021010_easy_ham.tar.bz2\n",
        "!tar -xjf 20021010_hard_ham.tar.bz2\n",
        "!tar -xjf 20021010_spam.tar.bz2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-12-02 09:19:32--  https://spamassassin.apache.org/old/publiccorpus/20021010_easy_ham.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 95.216.26.30, 95.216.24.32, 40.79.78.1, ...\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|95.216.26.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1677144 (1.6M) [application/x-bzip2]\n",
            "Saving to: ‘20021010_easy_ham.tar.bz2’\n",
            "\n",
            "20021010_easy_ham.t 100%[===================>]   1.60M  1.84MB/s    in 0.9s    \n",
            "\n",
            "2020-12-02 09:19:33 (1.84 MB/s) - ‘20021010_easy_ham.tar.bz2’ saved [1677144/1677144]\n",
            "\n",
            "--2020-12-02 09:19:33--  https://spamassassin.apache.org/old/publiccorpus/20021010_hard_ham.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 95.216.26.30, 95.216.24.32, 40.79.78.1, ...\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|95.216.26.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1021126 (997K) [application/x-bzip2]\n",
            "Saving to: ‘20021010_hard_ham.tar.bz2’\n",
            "\n",
            "20021010_hard_ham.t 100%[===================>] 997.19K  1.35MB/s    in 0.7s    \n",
            "\n",
            "2020-12-02 09:19:34 (1.35 MB/s) - ‘20021010_hard_ham.tar.bz2’ saved [1021126/1021126]\n",
            "\n",
            "--2020-12-02 09:19:34--  https://spamassassin.apache.org/old/publiccorpus/20021010_spam.tar.bz2\n",
            "Resolving spamassassin.apache.org (spamassassin.apache.org)... 95.216.26.30, 95.216.24.32, 40.79.78.1, ...\n",
            "Connecting to spamassassin.apache.org (spamassassin.apache.org)|95.216.26.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1192582 (1.1M) [application/x-bzip2]\n",
            "Saving to: ‘20021010_spam.tar.bz2’\n",
            "\n",
            "20021010_spam.tar.b 100%[===================>]   1.14M  1.26MB/s    in 0.9s    \n",
            "\n",
            "2020-12-02 09:19:36 (1.26 MB/s) - ‘20021010_spam.tar.bz2’ saved [1192582/1192582]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdH1XTepLjZ3"
      },
      "source": [
        "*The* data is now in the three folders `easy_ham`, `hard_ham`, and `spam`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A53Gw00fBLG2",
        "outputId": "d66f6bc8-2d71-4f75-b646-20e6f8634738"
      },
      "source": [
        "!ls -lah"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 4.0M\n",
            "drwxr-xr-x 1 root root 4.0K Dec  2 09:08 .\n",
            "drwxr-xr-x 1 root root 4.0K Dec  2 09:05 ..\n",
            "-rw-r--r-- 1 root root 1.6M Jun 29  2004 20021010_easy_ham.tar.bz2\n",
            "-rw-r--r-- 1 root root 998K Dec 16  2004 20021010_hard_ham.tar.bz2\n",
            "-rw-r--r-- 1 root root 1.2M Jun 29  2004 20021010_spam.tar.bz2\n",
            "drwxr-xr-x 1 root root 4.0K Nov 20 17:15 .config\n",
            "drwx--x--x 2  500  500 168K Oct 10  2002 easy_ham\n",
            "drwx--x--x 2 1000 1000  20K Dec 16  2004 hard_ham\n",
            "drwxr-xr-x 1 root root 4.0K Nov 13 17:33 sample_data\n",
            "drwxr-xr-x 2  500  500  36K Oct 10  2002 spam\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DGlWPVnSNzT7"
      },
      "source": [
        "###1. Preprocessing: \n",
        "1.\tNote that the email files contain a lot of extra information, besides the actual message. Ignore that for now and run on the entire text. Further down (in the higher-grade part), you will be asked to filter out the headers and footers. \n",
        "2.\tWe don’t want to train and test on the same data. Split the spam and the ham datasets in a training set and a test set. (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J2sllUWXKblD"
      },
      "source": [
        "#pre-processing code here\n",
        "import os\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import pandas as pd\n",
        "\n",
        "def get_file_list_from_dir(datadir):\n",
        "    all_files = os.listdir(os.path.abspath(datadir))\n",
        "    all_content = []\n",
        "    for file_name in all_files:\n",
        "      all_content.append(open(datadir + file_name, \"r\", errors='ignore').read())\n",
        "    return all_content\n",
        "\n",
        "# List of emails (one email = one string)\n",
        "easy_ham = get_file_list_from_dir(\"easy_ham/\")\n",
        "hard_ham = get_file_list_from_dir(\"hard_ham/\")\n",
        "spam = get_file_list_from_dir(\"spam/\")\n",
        "\n",
        "# Split data\n",
        "hamtrain,hamtest = train_test_split(easy_ham + hard_ham, \n",
        "                                    test_size=0.3, random_state=17)\n",
        "spamtrain,spamtest = train_test_split(spam, \n",
        "                                      test_size=0.3, random_state=17)\n",
        "\n",
        "hardhamtrain,hardhamtest = train_test_split(hard_ham, \n",
        "                                            test_size=0.3, random_state=17)\n",
        "easyhamtrain,easyhamtest = train_test_split(easy_ham, \n",
        "                                            test_size=0.3, random_state=17)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mnbrbI0_OKCF"
      },
      "source": [
        "###2. Write a Python program that: \n",
        "1.\tUses four datasets (`hamtrain`, `spamtrain`, `hamtest`, and `spamtest`) \n",
        "2.\tTrains a Naïve Bayes classifier (e.g. Sklearn) on `hamtrain` and `spamtrain`, that classifies the test sets and reports True Positive and False Negative rates on the `hamtest` and `spamtest` datasets. You can use `CountVectorizer` to transform the email texts into vectors. Please note that there are different types of Naïve Bayes Classifier in SKlearn ([Documentation here](https://scikit-learn.org/stable/modules/naive_bayes.html)). Test two of these classifiers that are well suited for this problem\n",
        "- Multinomial Naive Bayes  \n",
        "- Bernoulli Naive Bayes. \n",
        "\n",
        "Please inspect the documentation to ensure input to the classifiers is appropriate. Discuss the differences between these two classifiers. \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "38IVmw3QgAYo"
      },
      "source": [
        "def run_predictions(hamtrain, spamtrain, hamtest, spamtest, \n",
        "                    multi=MultinomialNB(), \n",
        "                    berno=BernoulliNB(), \n",
        "                    vectorizer=CountVectorizer()):\n",
        "  \n",
        "  # Transform data into word feature vector\n",
        "  train = vectorizer.fit_transform(hamtrain + spamtrain)\n",
        "\n",
        "  labels = ['ham']*len(hamtrain) + ['spam']*len(spamtrain)\n",
        "\n",
        "  test_ham = vectorizer.transform(hamtest)\n",
        "  test_spam = vectorizer.transform(spamtest)\n",
        "  test_set = vectorizer.transform(hamtest + spamtest)\n",
        "  test_labels = ['ham']*len(hamtest) + ['spam']*len(spamtest)\n",
        "\n",
        "  # Do math\n",
        "  multi.fit(train, labels)\n",
        "  berno.fit(train, labels)\n",
        "\n",
        "  prediction_multi_ham = multi.predict(test_ham)\n",
        "  prediction_berno_ham = berno.predict(test_ham)\n",
        "\n",
        "  prediction_multi_spam = multi.predict(test_spam)\n",
        "  prediction_berno_spam = berno.predict(test_spam)\n",
        "\n",
        "  # Show\n",
        "  unique, counts = np.unique(prediction_multi_ham, return_counts=True)\n",
        "  print(f\"Ham prediction for multinomial: {dict(zip(unique, counts))}\")\n",
        "  unique, counts = np.unique(prediction_berno_ham, return_counts=True)\n",
        "  print(f\"Ham prediction for bernoulli:   {dict(zip(unique, counts))}\")\n",
        "\n",
        "  print()\n",
        "\n",
        "  unique, counts = np.unique(prediction_multi_spam, return_counts=True)\n",
        "  print(f\"Spam prediction for multinomial: {dict(zip(unique, counts))}\")\n",
        "  unique, counts = np.unique(prediction_berno_spam, return_counts=True)\n",
        "  print(f\"Spam prediction for bernoulli:   {dict(zip(unique, counts))}\")\n",
        "  \n",
        "  print()\n",
        "\n",
        "  print(f\"Accuracy multinomial: {multi.score(test_set,test_labels):,.2f}\")\n",
        "  print(f\"Accuracy bernoulli: {berno.score(test_set, test_labels):,.2f}\")\n",
        "\n",
        "  print()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJERHSCcGNaW",
        "outputId": "fba9d0ef-4db3-4a65-a40d-5406c78b0c07"
      },
      "source": [
        "run_predictions(hamtrain, spamtrain, hamtest, spamtest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ham prediction for multinomial: {'ham': 836, 'spam': 5}\n",
            "Ham prediction for bernoulli:   {'ham': 839, 'spam': 2}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 12, 'spam': 139}\n",
            "Spam prediction for bernoulli:   {'ham': 110, 'spam': 41}\n",
            "\n",
            "Accuracy multinomial: 0.98\n",
            "Accuracy bernoulli: 0.89\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nI1bPDCvQxen"
      },
      "source": [
        "#### Discuss the differences between these two classifiers.\n",
        "The two classifiers that we use are Multinomial Naive Bayes and Bernoulli Naive Bayes. Both classifiers are commonly used for text classification but as we will see in this report, they will perform differently on the same input. \n",
        "\n",
        "This is because a Multinomial Naive Bayes classifier will consider all features (in our case, a feature is a single word) and how many times that feature appear. While a Bernoulli Naive Bayes classifier will only consider if a dataset has a feature or not and not how frequent that feature is. A Bernoulli classifier will also explicitly penalise a mail that does that have a feature otherwise common for a specific label.\n",
        "\n",
        "These differences between the different classifiers lead to Bernoulli having worse accuracy than the multinomial classifier. As we see in question 4, removing uncommon words will increase Bernoulli’s accuracy significantly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDFS3uFFUcS7"
      },
      "source": [
        "\n",
        "### 3.Run your program on \n",
        "-\tSpam versus easy-ham \n",
        "-\tSpam versus hard-ham."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gool_zb8Qzzy",
        "outputId": "d7d97d34-e824-4d6d-8d84-f0eeb7b8a88d"
      },
      "source": [
        "print(\"Predictions on easy-ham vs spam\")\n",
        "run_predictions(easyhamtrain, spamtrain, easyhamtest, spamtest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions on easy-ham vs spam\n",
            "Ham prediction for multinomial: {'ham': 766}\n",
            "Ham prediction for bernoulli:   {'ham': 764, 'spam': 2}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 15, 'spam': 136}\n",
            "Spam prediction for bernoulli:   {'ham': 72, 'spam': 79}\n",
            "\n",
            "Accuracy multinomial: 0.98\n",
            "Accuracy bernoulli: 0.92\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qt7ELzEqUfas",
        "outputId": "972d4c75-1c17-493a-cedc-7270209ffd92"
      },
      "source": [
        "print(\"Predictions on hard-ham vs spam\")\n",
        "run_predictions(hardhamtrain, spamtrain, hardhamtest, spamtest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predictions on hard-ham vs spam\n",
            "Ham prediction for multinomial: {'ham': 55, 'spam': 20}\n",
            "Ham prediction for bernoulli:   {'ham': 48, 'spam': 27}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 8, 'spam': 143}\n",
            "Spam prediction for bernoulli:   {'ham': 4, 'spam': 147}\n",
            "\n",
            "Accuracy multinomial: 0.88\n",
            "Accuracy bernoulli: 0.86\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKYkWGIrN4np"
      },
      "source": [
        "Both get a better spam prediction on the hard data, even to the point that bernoulli performs equally well to the multinomial prediction. As expected however, it is a bit harder to classify the ham correctly and as such both gets worse results in those regards.\n",
        "\n",
        "In the easy data the ham prediction is still almost flawless for both methods. However the spam prediction is worse for multinomial, but stronger for bernoulli (in comparison to when we used all ham data). It seems therefore that bernoulli performs a lot better when the data is split into the difficulty categories. Do note however that the spam prediction is still very bad and we have not yet seen any benefits to using bernoulli over multinomial for this task.\n",
        "\n",
        "**Our guesses as to why**\n",
        "\n",
        "Since the hard ham has more intricate and perhaps smaller differences from the spam, the algorithm is able to identify these small differences better. In the combined sample case the algorithm is flooded with easy ham that misconstrues the minute differences as something less useful to look at.\n",
        "\n",
        "In regards to ham it seems reasonable that it's harder to identify the ham since there are fewer samples. Furthermore the prior for the spam is a lot higher now that we have fewer cases and as such it's more likely for it to be overrepresented, just as how ham is over represented in both the easy ham and combined samples.\n",
        "\n",
        "Why the bernoulli distribution is better on easy ham rather than the combined samples, doesn't make sense at first. However since bernoulli distinguishes on features based on occurence instead of the amount of occurences, it likely performs better due to some features that are present in both the hard ham and spam, now being removed. Previously it would probably predict those samples to be ham, since they had a higher prior."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkfQWBB4UhYd"
      },
      "source": [
        "###4.\tTo avoid classification based on common and uninformative words it is common to filter these out. \n",
        "\n",
        "**a.** Argue why this may be useful. Try finding the words that are too common/uncommon in the dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6DLMMgy86R9i",
        "outputId": "123ff152-53ce-4ba5-df32-e8bbdbd2c1ed"
      },
      "source": [
        "def uncommunWords(freq, cutoff):\n",
        "  i=0\n",
        "  for elem in freq: \n",
        "    if elem[1] <= cutoff:\n",
        "      i+=1\n",
        "\n",
        "  return i\n",
        "\n",
        "def count_uncommon_common(data, return_res=False):\n",
        "  # Transform data into feature vector\n",
        "  vectorizer = CountVectorizer()\n",
        "  count = vectorizer.fit(data)\n",
        "  clump = count.transform(data)\n",
        "\n",
        "  # Count occurances\n",
        "  sumw = clump.sum(axis=0)\n",
        "  frequency = [(word, sumw[0,index]) for word, index in count.vocabulary_.items()]\n",
        "  frequency = sorted(frequency, key=lambda x: x[1], reverse=True)\n",
        "  \n",
        "  if return_res:\n",
        "    return frequency\n",
        "  else: \n",
        "    print(f\"The data had {len(data)} entries with {len(frequency)} different words\")\n",
        "    print(f\"The 10 most common words are:\")\n",
        "    print(*frequency[0:10])\n",
        "\n",
        "    print(f\"There are {uncommunWords(frequency, 1)} words, that only occur once in the data\")\n",
        "    print(f\"There are {uncommunWords(frequency, 5)} words that occur five times or less in the data\")\n",
        "    print()\n",
        "\n",
        "count_uncommon_common(hamtrain + spamtrain)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The data had 2310 entries with 82402 different words\n",
            "The 10 most common words are:\n",
            "('com', 46578) ('the', 27745) ('to', 26485) ('http', 20742) ('from', 20092) ('2002', 19752) ('td', 17555) ('for', 16455) ('net', 15600) ('with', 15512)\n",
            "There are 48189 words, that only occur once in the data\n",
            "There are 69873 words that occur five times or less in the data\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzobpb5WyQUa"
      },
      "source": [
        "Removing common and uncommon words might lead to a better prediction since we, in loose terms, are trying to classify a mail depending on what words the mail contains. For example, a word like `Chalmers` or `Unambiguous` might only appear a few times in ham-mails and no times in spam mails through the whole training-set. The classifier would therefor \"think\" that a mail containing `Chalmers` cannot be a spam mail since no spam mail in the training-set contained `Chalmers`. \n",
        "\n",
        "This is a larger problem for the Bernoulli classifier since it only considers if a certain mail has a specific feature or not, this will be discussed in more detail in the latter part of the question.\n",
        "\n",
        "For common words it is the opposite case, if a word appears in almost every mail it will be impossible to classify the mail based on that word and might mislead a calculation. For example, `com` appears on average 20 times per email.\n",
        "\n",
        "Above we have printed the 10 most common words and the count of uncommon words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgrAMhmWjOca"
      },
      "source": [
        "**b.** Use the parameters in Sklearn’s `CountVectorizer` to filter out these words. Update the program from point 3 and run it on your data and report your results.\n",
        "\n",
        "You have two options to do this in Sklearn: either using the words found in part (a) or letting Sklearn do it for you. Argue for your decision-making."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ggH4Ldas5YPO",
        "outputId": "227e934c-c428-4f8c-e8da-f01ca408394c"
      },
      "source": [
        "print(\"1. No removal of common or uncommon words\")\n",
        "run_predictions(hamtrain, spamtrain, hamtest, spamtest)\n",
        "\n",
        "print(\"---------------------------\")\n",
        "print(\"2. Remove both common and uncommon words. Using sklearn, min_df = 5, max_df=0.8 \")\n",
        "print(\"Removed words that appeared less than 5 times or word that appeared in more than 80% of all emails\")\n",
        "run_predictions(hamtrain, spamtrain, hamtest, spamtest, \n",
        "                vectorizer=CountVectorizer(min_df=5, max_df=0.8))\n",
        "\n",
        "# Creat list of uncommon words\n",
        "freq = count_uncommon_common(hamtrain + spamtrain, return_res=True)\n",
        "stopwords = list(map(lambda x: x[0], filter(lambda x: x[1] < 5, freq)))\n",
        "\n",
        "print(\"---------------------------\")\n",
        "\n",
        "print(f\"3. Remove uncommonwords, using own stopwords list, {(len(stopwords)/len(freq)*100):,.2f} % of data was removed\")\n",
        "print(\"Removed words that appeared less than 5 times.\")\n",
        "run_predictions(hamtrain, spamtrain, hamtest, spamtest, \n",
        "                vectorizer=CountVectorizer(stop_words=stopwords))\n",
        "\n",
        "print(\"---------------------------\")\n",
        "print('4. Remove common english words, using sklearns stopword set, \"english\"')\n",
        "run_predictions(hamtrain, spamtrain, hamtest, spamtest, \n",
        "                vectorizer=CountVectorizer(stop_words=\"english\"))\n",
        "\n",
        "print(\"---------------------------\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1. No removal of common or uncommon words\n",
            "Ham prediction for multinomial: {'ham': 836, 'spam': 5}\n",
            "Ham prediction for bernoulli:   {'ham': 839, 'spam': 2}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 12, 'spam': 139}\n",
            "Spam prediction for bernoulli:   {'ham': 110, 'spam': 41}\n",
            "\n",
            "Accuracy multinomial: 0.98\n",
            "Accuracy bernoulli: 0.89\n",
            "\n",
            "---------------------------\n",
            "2. Remove both common and uncommon words. Using sklearn, min_df = 5, max_df=0.8 \n",
            "Removed words that appeared less than 5 times or word that appeared in more than 80% of all emails\n",
            "Ham prediction for multinomial: {'ham': 825, 'spam': 16}\n",
            "Ham prediction for bernoulli:   {'ham': 814, 'spam': 27}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 1, 'spam': 150}\n",
            "Spam prediction for bernoulli:   {'spam': 151}\n",
            "\n",
            "Accuracy multinomial: 0.98\n",
            "Accuracy bernoulli: 0.97\n",
            "\n",
            "---------------------------\n",
            "3. Remove uncommonwords, using own stopwords list, 82.85 % of data was removed\n",
            "Removed words that appeared less than 5 times.\n",
            "Ham prediction for multinomial: {'ham': 827, 'spam': 14}\n",
            "Ham prediction for bernoulli:   {'ham': 816, 'spam': 25}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 3, 'spam': 148}\n",
            "Spam prediction for bernoulli:   {'ham': 1, 'spam': 150}\n",
            "\n",
            "Accuracy multinomial: 0.98\n",
            "Accuracy bernoulli: 0.97\n",
            "\n",
            "---------------------------\n",
            "4. Remove common english words, using sklearns stopword set, \"english\"\n",
            "Ham prediction for multinomial: {'ham': 838, 'spam': 3}\n",
            "Ham prediction for bernoulli:   {'ham': 839, 'spam': 2}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 12, 'spam': 139}\n",
            "Spam prediction for bernoulli:   {'ham': 113, 'spam': 38}\n",
            "\n",
            "Accuracy multinomial: 0.98\n",
            "Accuracy bernoulli: 0.88\n",
            "\n",
            "---------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQi1MLPFsNce"
      },
      "source": [
        "*Above we've tried different techniques and combinations of removing common/uncommon words*\n",
        "\n",
        "From both test 2 and 4 we see that removing common words have no significant affect on the result, this might be because either word appears in all mails or in less than 80% of mail. Then filtering out these features will not influence the classification.\n",
        "\n",
        "In test 2 and 3 when uncommon words are removed, we see that multinomial is not affected in a significant way. But the Bernoulli classifiers accuracy increases by almost 10 % units. This is because of the differences in classifiers that were discussed in question 2 since Bernoulli only \"looks\" if a certain mail contain a feature or not. Hence if an uncommon word from the training set appeared in a test-mail, that mail might be misclassified."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xPq73IU6QADV",
        "outputId": "4eb89d79-219d-40e4-e4c2-1f18bfaa9bc3"
      },
      "source": [
        "# Check for uncommon words in testdata\n",
        "freq_test = dict(count_uncommon_common(hamtest + spamtest, return_res=True))\n",
        "\n",
        "uncommon_in_test = len(list(filter(lambda x: x[0] in freq_test, stopwords)))\n",
        "\n",
        "print(f\"There are {uncommon_in_test} uncommon words from the traing set that appear in the test set\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 0 uncommon words from the traing set that appear in the test set\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4RCfZE3YR0Ja"
      },
      "source": [
        "But from the code above we see that none of the uncommon words from the training reappear in the test set. Hence Bernoulli’s increase in accuracy might instead be because Bernoulli penalise a mail that does not have a feature common for a type of mail. For example, if `Chalmers` only appear once in all training mails and that mail is a ham mail, Bernoulli will think it less likely that a mail not containing `Chalmers` is a ham mail. \n",
        "\n",
        "This is probably the reason that most of the emails that Bernoulli miss-classifies are spam mails. Since these often contain random weird strings of characters like the examples below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zb8CYSaSTwtZ",
        "outputId": "d522f447-62c5-4ea6-9e94-8435f717c152"
      },
      "source": [
        "print(f\"Example of words that appear once: {stopwords[-5:]}\")\n",
        "print(f\"Example of words that appear five times: {stopwords[:5]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of words that appear once: ['s12pd7lmwm', 'muatiounbwshcouk8l2zvbnq', 'pc9wpg0kdqo8l2jvzhk', 'dqoncjwvahrtbd4', '200209140414']\n",
            "Example of words that appear five times: ['01t00', 'footage', 'terrific', 'monkeys', 'blacklist']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3DBsIKPqUpjB"
      },
      "source": [
        "Here we chose to define an uncommon word as when it appeared less than 5 times and a common word as if it appeared in more than 80 % of all documents. We chose these parameters since they gave a high accuracy on the test set while removing as little data as possible. But here the test set is used to set a parameter which can lead to an overfitting for the test set. A better way to choose parameters for a certain application would be to use a separate validation set, where the parameters are optimized on the validation set and the model is then evaluated on the test set.\n",
        "\n",
        "*In some of our runs (before specifying random_state in or test train split) the accuracy of the multinomial classifier increased by a few % units, but this depends on the exact split of train and test data, which will be discussed in question 5*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zcyVfOZFU4F_"
      },
      "source": [
        "###5. Eeking out further performance\n",
        "Filter out the headers and footers of the emails before you run on them. The format may vary somewhat between emails, which can make this a bit tricky, so perfect filtering is not required. Run your program again and answer the following questions: \n",
        "-\tDoes the result improve from 3 and 4? \n",
        "- The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies? \n",
        "- What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages? \n",
        "\n",
        "Re-estimate your classifier using `fit_prior` parameter set to `false`, and answer the following questions:\n",
        "- What does this parameter mean?\n",
        "- How does this alter the predictions? Discuss why or why not."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s_nyGug9U4f3",
        "outputId": "94943cad-9925-41c1-ee0f-b21b5236941a"
      },
      "source": [
        "import email\n",
        "\n",
        "# preprocess\n",
        "\n",
        "def get_body_from_msg(msg):\n",
        "  output = \"\"\n",
        "  if msg.is_multipart():\n",
        "      for payload in msg.get_payload():\n",
        "          if payload.is_multipart():\n",
        "            output + \" \" + get_body_from_msg(payload)\n",
        "            continue \n",
        "\n",
        "          output + \" \" + payload.get_payload()\n",
        "      return output\n",
        "  else:\n",
        "      return msg.get_payload()\n",
        "\n",
        "# List of emails (one email = one string)\n",
        "easy_ham = get_file_list_from_dir(\"easy_ham/\")\n",
        "hard_ham = get_file_list_from_dir(\"hard_ham/\")\n",
        "spam = get_file_list_from_dir(\"spam/\")\n",
        "\n",
        "no_head_easy_ham = list(map(get_body_from_msg, \n",
        "                            map(email.message_from_string, easy_ham)))\n",
        "no_head_hard_ham = list(map(get_body_from_msg, \n",
        "                            map(email.message_from_string, hard_ham)))\n",
        "no_head_spam = list(map(get_body_from_msg, \n",
        "                        map(email.message_from_string, spam)))\n",
        "\n",
        "no_head_hamtrain, no_head_hamtest = train_test_split(no_head_easy_ham +\n",
        "                                                     no_head_hard_ham, \n",
        "                                                     test_size=0.3)\n",
        "\n",
        "no_head_spamtrain, no_head_spamtest = train_test_split(no_head_spam, \n",
        "                                                       test_size=0.3)\n",
        "\n",
        "no_head_easyhamtrain, no_head_easyhamtest = train_test_split(no_head_easy_ham, \n",
        "                                                             test_size=0.3)\n",
        "\n",
        "no_head_hardhamtrain, no_head_hardhamtest = train_test_split(no_head_hard_ham,\n",
        "                                                             test_size=0.3)\n",
        "\n",
        "count_uncommon_common(no_head_hamtrain + no_head_spamtrain)\n",
        "\n",
        "run_predictions(no_head_hamtrain, no_head_spamtrain, no_head_hamtest, no_head_spamtest)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The data had 2310 entries with 48317 different words\n",
            "The 10 most common words are:\n",
            "('the', 25686) ('http', 17665) ('com', 16738) ('to', 16089) ('td', 12932) ('of', 12581) ('and', 12555) ('font', 10141) ('width', 9981) ('www', 9579)\n",
            "There are 24961 words, that only occur once in the data\n",
            "There are 38607 words that occur five times or less in the data\n",
            "\n",
            "Ham prediction for multinomial: {'ham': 841}\n",
            "Ham prediction for bernoulli:   {'ham': 837, 'spam': 4}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 42, 'spam': 109}\n",
            "Spam prediction for bernoulli:   {'ham': 107, 'spam': 44}\n",
            "\n",
            "Accuracy multinomial: 0.96\n",
            "Accuracy bernoulli: 0.89\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwNakEc2zb83"
      },
      "source": [
        "- Does the result improve from 3 and 4?\n",
        "\n",
        "The result is worse when we remove the boiler plate and header. This means that the information there actually has some value. The thing of value could for example be the subject of the mail, who it is sent from (some types of mailadresses could be more common in spam mails, for example the \".com\" could be something else or it could be that they try to send from mails such as \"xxx@whitehouse.xxx\") or simply the amount of information that is presented.\n",
        "\n",
        "- The split of the data set into a training set and a test set can lead to very skewed results. Why is this, and do you have suggestions on remedies?\n",
        "\n",
        "In our cases the results have never been huge since we have used sklearns built in split method and because of us splitting the spam and ham separately, so that we get a proportional amount in both sets. The most notable difference is that in some splits the \"easy ham vs spam\" and \"hard ham vs spam\" gets different scores to the point that they alter which is better. It is never by a huge margin and in the other cases the difference is a 1-2% on the score at most. In order to remedy problems in the test case split you can try to run on different splits and get a more average result that way. We can also try to use a third validity set to optimize the hyper-parameters in the split. A more common way is probably to prune the input to remove some outliers which might be the cause of the skew. Lastly we can try to get more data points so that it is less likely to leave any key features out.\n",
        "\n",
        " - What do you expect would happen if your training set were mostly spam messages while your test set were mostly ham messages?\n",
        "\n",
        "The prior would be skewed in the opposite direction, which would lead to bad predictions. Furthermore it would probably lead to the a similar affect a what we get in #3 for the hard ham bernoulli distribution. There it would become better at identifying spam, but the ham prediction would go way down."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66voWfPELxq5",
        "outputId": "6040d555-87a5-4deb-b3ac-ebba338a1f3e"
      },
      "source": [
        "print(\"Fit_prior = False\")\n",
        "run_predictions(hamtrain, spamtrain, hamtest, spamtest, \n",
        "                multi = MultinomialNB(fit_prior=False),\n",
        "                berno = BernoulliNB(fit_prior=False))\n",
        "\n",
        "print(\"Fit_prior = True\")\n",
        "run_predictions(hamtrain, spamtrain, hamtest, spamtest, \n",
        "                multi = MultinomialNB(fit_prior=True),\n",
        "                berno = BernoulliNB(fit_prior=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fit_prior = False\n",
            "Ham prediction for multinomial: {'ham': 836, 'spam': 5}\n",
            "Ham prediction for bernoulli:   {'ham': 839, 'spam': 2}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 12, 'spam': 139}\n",
            "Spam prediction for bernoulli:   {'ham': 108, 'spam': 43}\n",
            "\n",
            "Accuracy multinomial: 0.98\n",
            "Accuracy bernoulli: 0.89\n",
            "\n",
            "Fit_prior = True\n",
            "Ham prediction for multinomial: {'ham': 836, 'spam': 5}\n",
            "Ham prediction for bernoulli:   {'ham': 839, 'spam': 2}\n",
            "\n",
            "Spam prediction for multinomial: {'ham': 12, 'spam': 139}\n",
            "Spam prediction for bernoulli:   {'ham': 110, 'spam': 41}\n",
            "\n",
            "Accuracy multinomial: 0.98\n",
            "Accuracy bernoulli: 0.89\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nEF9WSUHsfyO"
      },
      "source": [
        "#### What does this parameter mean?\n",
        "\n",
        "From the documentation we learn that fit_prior indicates \"*Whether to learn class prior probabilities or not. If false, a uniform prior will be used.*\" where a prior for a given class is the proportion that the class occurs over the total number of samples. A uniform prior in this case should indicate that the prior would be 1/2 for both cases, which could be smart to do if we assume the samples are unnaturally skewed towards the wrong direction.\n",
        "\n",
        "#### How does this alter the predictions? Discuss why or why not.\n",
        "\n",
        "Our results indicates that fit_prior does not affect the results significantly in any direction. We also tried it on easy and hard ham, which gave the same result. This seems to indicate that a lot of the previous assumptions we have made regarding the prior might be incorrect, especially in regards to #3. It might still have an impact on for example the case where \"*your training set were mostly spam messages while your test set were mostly ham messages*\", but it also means that we could simply turn off fit_prior to test if the results from the prediction would be better."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ND6FKoexVAhW"
      },
      "source": [
        "### What to report and how to hand in.\n",
        "\n",
        "- You will need to clearly report all results in the notebook in a clear and appropriate way, either using plots or code output (f.x. \"print statements\"). \n",
        "- The notebook must be reproducible, that means, we must be able to use the `Run all` function from the `Runtime` menu and reproduce all your results. **Please check this before handing in.** \n",
        "- Save the notebook and share a link to the notebook (Press share in upper left corner, and use `Get link` option. **Please make sure to allow all with the link to open and edit.**\n",
        "- Edits made after submission deadline will be ignored, graders will recover the last saved version before deadline from the revisions history.\n",
        "- **Please make sure all cells are executed and all the output is clearly readable/visible to anybody opening the notebook.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bI3z_spVacz"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}